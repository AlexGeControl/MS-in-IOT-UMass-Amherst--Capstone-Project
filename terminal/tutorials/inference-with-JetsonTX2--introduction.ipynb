{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"> <img src=\"files/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Embedded Applications\n",
    "\n",
    "## Set up Jetson TX2\n",
    "\n",
    "Important - READ FIRST!\n",
    "This lab requires some setup:\n",
    "\n",
    "Open a terminal window (Ctrl+Alt+T) on Jetson to initialize a few tools and models. They are going to take about 10 minutes to run, so execute them as the instructor introduces the course.\n",
    "\n",
    "### The password to the first command (below) is 'nvidia'. Run it from the home directory:\n",
    "\n",
    "<pre>sudo apt-get install -y git cmake </pre>\n",
    "\n",
    "### This second command (below) can be copied and pasted as one block:\n",
    "\n",
    "Copying and pasting will appear to paste only the first line, however, once that command has been run, the second will appear.\n",
    "\n",
    "```   \n",
    "git clone http://github.com/dusty-nv/jetson-inference  \n",
    "cd jetson-inference  \n",
    "mkdir build  \n",
    "cd build  \n",
    "cmake ../  \n",
    "make  \n",
    "cd aarch64/bin \n",
    "```   \n",
    "\n",
    "You are now in a folder that contains applications and models that we will learn with, but that you can use on your own if you repeat these steps on your own Jetson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployed Models\n",
    "\n",
    "### Pre-Input and Post-Output\n",
    "\n",
    "While what is inside of a deep neural network may be complex, at their core, they are simply functions. They take some input and generate some output. \n",
    "\n",
    "![](functionmachine.PNG)\n",
    "\n",
    "For the sake of this conversation, we're going to assume that our network has been architected and trained successfully, and focus specifically on the skill of *deployment*. \n",
    "\n",
    "To successfully *deploy* a trained model, we have two initial jobs.\n",
    "\n",
    "1) Our first job is to provide our model the an **input** that it expects.  \n",
    "2) Our second job is to provide our end user an **output** that is useful.  \n",
    "\n",
    "The input that our network expects is determined both by its architecture and by *how* it was trained. Deployment requires writing code to convert the input we have to the input the network expects. The output that our network generates is determined by its architecture and what it learned. Deployment requires writing code to convert the output that is generated to the output our end user expects. \n",
    "\n",
    "For the beginning of this lab, we will look at a few applications that use trained models. Our job will be to identify four characteristics:\n",
    "\n",
    "1) The input of the *application*.  \n",
    "2) The input of the *network*.  \n",
    "3) The output of the *network.*  \n",
    "4) The output of the *application.*  \n",
    "\n",
    "If those 4 characteristics are identified, deployment consists of writing code in any programming language from (1) to (2) and from (3) to (4).\n",
    "\n",
    "**Challenge: While it runs, identify the inputs (1) and outputs (4) of the application <code>imagenet-camera</code>. Once you have identified them, feel free to experiment with the application to see how (and when) it works.**\n",
    "\n",
    "Note: To exit the application, click the red (x) in the top left. \n",
    "\n",
    "In Jetson's terminal, from the directory where you left off (~/jetson-inference/build/aarch/bin), run imagenet-camera with the command: \n",
    "\n",
    "<code>./imagenet-camera</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](filecabinet.png)\n",
    "\n",
    "You can see that <code>imagenet-camera</code>: <pre>\n",
    "\n",
    "Takes as input, image data from the camera.\n",
    "Creates as output, the input image's most likely classification and its likelihood printed on top of the image.</pre>\n",
    "\n",
    "To start to understand how this was built, our next job is to identify the expected input (2) and generated output (3) of the *trained neural network.*\n",
    "\n",
    "### Expected Input\n",
    "\n",
    "To successfully deploy our trained network, we have to convert the application's input to the input expected by the neural network.\n",
    "\n",
    "#### Examining our network architecture\n",
    "\n",
    "The information we care about should be pretty easy to find. We want to know the *dimensions* of the *input* layer. Each network has a file (which you'll need later), that describes the network in a human readable format. We're just going to look at the first layer that has dimensions listed.\n",
    "\n",
    "The network used in imagenet-camera, AlexNet was defined in the framework Caffe. The first layer is defined below. It tells us that AlexNet expects an input of the dimensions 227X227X3. (The \"10\" means that it will take 10 images at a time.) \n",
    "\n",
    "```\n",
    "name: \"AlexNet\"\n",
    "layer {\n",
    "  name: \"data\"\n",
    "  type: \"Input\"\n",
    "  top: \"data\"\n",
    "  input_param { shape: { dim: 10 dim: 3 dim: 227 dim: 227 } }\n",
    "}\n",
    "```\n",
    "\n",
    "This was found at the very top of the definition file. See how easy it is to find by examining the whole thing [here](https://github.com/BVLC/caffe/blob/master/models/bvlc_alexnet/deploy.prototxt).\n",
    "\n",
    "#### Application input to network input\n",
    "\n",
    "The input to the application is the raw camera footage from Jetson. Examining Jetson's documentation informs us that the camera captures 720p (1280X720X3) images. \n",
    "\n",
    "**For successful deployment, code was written (C++ is this case) to preprocess the image from 1280X720X3 to 227X227X3.**\n",
    "\n",
    "Note: There could be other constraints than size. Assess format (most networks expect raw pixels (RGB)) and any preprocessing that was done during training (often the mean image from a dataset is subtracted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output\n",
    "\n",
    "To successfully build a useful application, we have to convert the network's output to an output that is useful to a user.\n",
    "\n",
    "#### Examining our network architecture\n",
    "\n",
    "Examining the last layer of our network architecture (the bottom of the same file) tells us that Alexnet generates a 1000 unit vector. What the 1000 numbers represent demands a bit more specialized knowledge. In this case, the layer \"type\" is \"Softmax.\" A quick internet search of the word \"softmax\" returns a description of how to use softmax.\n",
    "\n",
    "In summary, each of the 1000 elements represents the probability that the image belongs to each class. \n",
    "\n",
    "```\n",
    "layer {\n",
    "  name: \"fc8\"\n",
    "  type: \"InnerProduct\"\n",
    "  bottom: \"fc7\"\n",
    "  top: \"fc8\"\n",
    "  param {\n",
    "    lr_mult: 1\n",
    "    decay_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    lr_mult: 2\n",
    "    decay_mult: 0\n",
    "  }\n",
    "  inner_product_param {\n",
    "    num_output: 1000\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"prob\"\n",
    "  type: \"Softmax\"\n",
    "  bottom: \"fc8\"\n",
    "  top: \"prob\"\n",
    "}\n",
    "```\n",
    "\n",
    "Again, you can see that this just came from the bottom of the network definition file, [here](https://github.com/BVLC/caffe/blob/master/models/bvlc_alexnet/deploy.prototxt).\n",
    "\n",
    "#### Network Output to Application Output\n",
    "\n",
    "The output of the application is what you saw, the input image's most likely classification and its likelyhood printed on top of the image. \n",
    "\n",
    "To go from the 1000 element softmax output, the C++ program identified the element of the output with the largest value, used a reference file, \"labels.txt,\" to identify which class that element represented, and displayed both the class title and that element's value written as a %. (It also chose a font and a location).\n",
    "\n",
    "Below is a diagram that represents the neural network as a function with an input and an output and shows in pseudo-code the logical steps that needed to be written before and after the network for a successful deployment.\n",
    "\n",
    "![](fmachinepsuedocode.PNG)\n",
    "\n",
    "While the code is not the key takeaway, if you are interested in seeing what this looks like in C++, you can examine the application [here](https://github.com/dusty-nv/jetson-inference/blob/master/imagenet-camera/imagenet-camera.cpp).\n",
    "\n",
    "Let's try again. Here, we'll look at a second application and work to identify the four same characteristics:  \n",
    "\n",
    "1) The input of the *application*.  \n",
    "2) The input of the *network*.  \n",
    "3) The output of the *network.*  \n",
    "4) The output of the *application.*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection Application\n",
    "\n",
    "**Challenge: Identify the input (1)  and output (4) of detectnet-camera. Run the application below and stand in front of the camera to visualize the input and output of the application. When done, exit the video stream.**\n",
    "\n",
    "Note: This application may take a few minutes to \"build CUDA engine.\"\n",
    "\n",
    "<code>./detectnet-camera</code>\n",
    "\n",
    "![](peddetect.png)\n",
    "\n",
    "You can see that this application takes the same input as imagenet-camera but instead outputs the input image with a blue rectangle covering people.\n",
    "\n",
    "**Double challenge: Examine the model architecture file to determine the input (2) and output (3) of the network that enables it to detect pedestrians from the top and bottom of the architecture file [here](https://github.com/dusty-nv/jetson-inference/blob/master/data/networks/detectnet.prototxt).**\n",
    "\n",
    "To review the last section, we'll create another function diagram of our deployment. In the next section, you'll have to find this information yourself, so try it with us. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](fmachineobject.PNG)\n",
    "\n",
    "### What constitutes a \"learned\" function\n",
    "\n",
    "A trained neural network consists of two components:\n",
    "\n",
    "1) A description of the network architecture - established prior to training  \n",
    "2) A index of learned parameters (weights) - established during training  \n",
    "\n",
    "We explored the network architecture in the previous section. In this section, we will see how we can use the same architecture in the same application, but replace the weights, to create an entirely new capability.\n",
    "\n",
    "**Explore: Replace the learned parameters in detectnet-camera with the learned parameters from a different dataset to prove that the application can harness different models as long as the model architectures share input and output layers.**\n",
    "\n",
    "Try on Jetson:\n",
    "\n",
    "./detectnet-camera facenet\n",
    "\n",
    "\n",
    "You can see that Detectnet was architected to learn to detect objects within an image, and then trained to apply that skillset to faces or pedestrians, etc. In deployment, since the input and output formats are identical, this application doesn't need to care whether it's overlaying blue bounding boxes on top of faces, pedestrians, dogs, bottles, etc. All the application needs to know is to draw a blue box from (x1,y1) to (x2,y2).\n",
    "\n",
    "Here are other options that were in the repo you downloaded. Feel free to play.\n",
    "\n",
    "- <code>./detectnet-camera coco-bottle</code> detects bottles/soda cans in the camera\n",
    "- <code>./detectnet-camera coco-dog</code> detects dogs in the camera (trust us, it works)  \n",
    "- <code>./detectnet-camera multiped</code> runs using multi-class pedestrian/luggage detector  \n",
    "- <code>./detectnet-camera pednet</code> runs using original single-class pedestrian detector  \n",
    "- <code>./detectnet-camera facenet</code> runs using facial recognition network  \n",
    "- <code>./detectnet-camera</code> by default, program runs using multiped  \n",
    "\n",
    "### Summary\n",
    "\n",
    "So far we have learned:\n",
    "\n",
    "1) Deployment is the task of using a deep neural network to some application.  \n",
    "2) To successfully deploy a network, we must understand the network's input and output.  \n",
    "3) A deployed neural network has two components, the model architecture and the learned weights.  \n",
    "\n",
    "The applications we've been working with and more are available from [this repo.](https://github.com/dusty-nv/jetson-inference)\n",
    "\n",
    "Next, we'll deploy a new model that has the same inputs and outputs as the model deployed in <code>imagenet-camera</code> to learn to go from data to deployment.\n",
    "\n",
    "When the class is ready, go to the [next notebook.](Deploying%20Custom%20Model.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
