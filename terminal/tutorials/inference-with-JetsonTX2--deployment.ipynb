{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](NVIDIA.DL.workflow.PNG)\n",
    "\n",
    "\n",
    "\n",
    "## Let's train the model:\n",
    "\n",
    "Next, we're going to train our own model.\n",
    "\n",
    "\n",
    "If you've trained image classification models using DIGITS before, the short instructions are:  \n",
    "\n",
    "* Train the MNIST dataset stored at: <code>/data/mnist_numbers/train_invert</code> as <code>28X28</code> and <code>Grayscale</code>\n",
    "* Using the <code>LeNet</code> Model  \n",
    "* For <code>5 epochs</code>\n",
    "\n",
    "\n",
    "### [Open DIGITS](https://classroom.udacity.com/nanodegrees/nd209/parts/dad7b7cc-9cce-4be4-876e-30935216c8fa/modules/4899a747-7c0d-4f40-9ab8-4f2eaf27a810/lessons/94be81d0-3186-4a4d-81a7-bd868a75da59/concepts/c7b9a470-ff7e-4897-a878-946b713bfcac?contentVersion=1.0.0&contentLocale=en-us)\n",
    "\n",
    "If you'd like a more full set of instructions, go [to this notebook.](Digits%20Instructions.ipynb)\n",
    "\n",
    "<a id='digitsreturn'></a>\n",
    "\n",
    "\n",
    "\n",
    "## Download the model:\n",
    "\n",
    "Now that we have created a model that can classify handwritten digits, we're going to download the model to deploy it to the \"imagenet-camera\" application.\n",
    "\n",
    "Save the model we just trained, \"DIGITStoJetson,\" to the Jetson by selecting \"Download Model\" as shown below.\n",
    "\n",
    "![](download.PNG)\n",
    "\n",
    "\n",
    "<b>Note:</b> make sure that the file downloads completely. You may have to give permission to \"keep\" the file.\n",
    "![](Digits.download.warning.png)\n",
    "\n",
    "## Extracting and locating your \"model\"\n",
    "\n",
    "### Extracting downloaded files\n",
    "\n",
    "To extract the model to a location where we can find it:\n",
    "\n",
    "1)  Go back to or reopen a terminal window: <pre>(ctrl-alt-t)</pre>\n",
    "2)  Create an environment variable to point to the folder where you will put the extracted files with: <pre>NET=~/jetson-inference/build/aarch64/bin/networks/digitstojetson</pre>\n",
    "3) Make a directory there with:  \n",
    "```mkdir $NET```  \n",
    "\n",
    "4) Change ###YOUR_FILE## below to the file you downloaded (likely starting with the current year) and then run the following command to extract the files to the NET directory:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tar -xvf ~/Downloads/##YOUR_FILE##.tar.gz -C $NET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching downloaded files to application parameters\n",
    "\n",
    "Let's work identify where to find each parameter that makes up your model. You'll need to run the following command with each ##FIXME## replaced.\n",
    "\n",
    "Hints: \n",
    "1. Prototxt, model, and labels are all files in the NET folder you just created. Find them with: <pre>ls $NET</pre>\n",
    "2. The input_blob is the *name* of the input layer and the output_blob is the *name* of the output layer. Find them in the deploy.prototxt file with:  \n",
    "  \n",
    "    ```vi $NET/deploy.prototxt```\n",
    "\n",
    "\n",
    "#### Challenge: Replace each #FIXME# below with the appropriate parameter, then copy, paste, and run this command in the terminal in the following location: <pre>~/jetson-inference/build/aarch64/bin</pre>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "./imagenet-camera \\\n",
    "--prototxt=$NET/#FIXME# \\\n",
    "--model=$NET/#FIXME# \\\n",
    "--labels=$NET/#FIXME# \\\n",
    "--input_blob=#FIXME# \\\n",
    "--output_blob=#FIXME#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a solution, see [here](#command)\n",
    "<a id = 'commandreturn'></a>\n",
    "\n",
    "\n",
    "#### At this point we have done the following tasks:\n",
    "\n",
    "* Trained your model using the LeNet network with the MNIST dataset\n",
    "* Downloaded the trained model\n",
    "* Extracted and copied the model to the correct location and setup the environment variable\n",
    "* Replaced the model in imagenet-camera\n",
    "\n",
    "\n",
    "## Running and testing your custom application\n",
    "\n",
    "\n",
    "##### Questions:\n",
    "* Does the application recognize handwritten digits?  (Try a sharpee on a piece of paper)\n",
    "* Does the application recognize its own data through the camera? (Look up MNIST on  your phone or another computer and show the camera).\n",
    "\n",
    "\n",
    "#### Here are examples of what you might see\n",
    "\n",
    "####  Using MNIST data\n",
    "![](result.imagenet.camera.mnist.image.png)\n",
    "\n",
    "\n",
    "####  Using a handwritten digit\n",
    "![](result.imagenet.camera.handwritten.image.2.jpg)\n",
    "\n",
    "\n",
    "You can see we've got a lot of work to do. Next, we'll what else we have control over for successful deployment. In order to assess why this works with images from our dataset, but not from \"handwritten digits\" as claimed motivates a conversation about overfitting, training on a diverse dataset, and the viability of classifying images in the wild without first localizing/detecting. However, what you do see is that you can build whatever you want AROUND a deployed deep neural network, given that you:\n",
    "\n",
    "- Have a model architecture\n",
    "- Have trained weights\n",
    "- Have any supplementary docs, like labels\n",
    "- Know the input and output shapes and expectations. \n",
    "\n",
    "## Next steps\n",
    "\n",
    "The only limit to what you can build *using* deployed neural networks is your programming knowledge. Since there are many places to learn to code, this lab studied deployment through existing applications. You now know:\n",
    "\n",
    "- How to preprocess your input to match the input expected by the network\n",
    "- How to postprocess your network's output to match the end user\n",
    "- The difference in workflow between changing functionality of an application with new weights, architectures, or surrounding logic\n",
    "\n",
    "Hopefully you gained a deeper understanding of the role of deep learning *within* applications.\n",
    "\n",
    "At this point, there are two directions you could go with your learning. \n",
    "\n",
    "1) The first would be to build and experiment with other embedded deep learning applications through [this repo](https://github.com/dusty-nv/jetson-inference).  \n",
    "2) The second would be to learn to deploy more customized models and control deployment performance with [TensorRT](https://devblogs.nvidia.com/parallelforall/deploying-deep-learning-nvidia-tensorrt/).\n",
    "\n",
    "What will you build!?\n",
    "\n",
    "# Appendix\n",
    "\n",
    "<a id = 'command'></a>\n",
    "\n",
    "### Run Imagenet-Camera solution\n",
    "\n",
    "Copy and paste the following into the directory that contains imagenet-camera:\n",
    "\n",
    "```./imagenet-camera \\  \n",
    "--prototxt=$NET/deploy.prototxt \\  \n",
    "--model=$NET/snapshot_iter_7035.caffemodel \\  \n",
    "--labels=$NET/labels.txt \\  \n",
    "--input_blob=data \\  \n",
    "--output_blob=softmax```\n",
    "\n",
    "Once it is running, return to the lab [here.](#commandreturn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
